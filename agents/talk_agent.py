import os
from dataclasses import dataclass
from typing import List, Literal
from pydantic_ai import Agent, RunContext
from utils.llm import chat_model
from utils.novel import split_sentences
from utils.tts import (
    generate_sentence_audio_and_srt, 
    merge_audio_files, 
    merge_srt_files
)


@dataclass
class TalkAgentDeps:
    script: str
    scene_id: int
    novel_content: str


@dataclass
class TalkAgentOutput:
    text: str = ""
    voice_type: Literal["male", "female", "narrator"] = "narrator"


talk_agent = Agent(
    model=chat_model,
    deps_type=TalkAgentDeps,
    output_type=List[TalkAgentOutput],
)


@talk_agent.instructions
def analyze_voice_assignment(ctx: RunContext[TalkAgentDeps]) -> str:
    """ä¸ºå·²ç»åˆ‡åˆ†å¥½çš„å¥å­åˆ†é…éŸ³è‰²"""
    scripts = ctx.deps.script
    novel_content = ctx.deps.novel_content
    scene_id = ctx.deps.scene_id
    sentences = '\n'.join(split_sentences(scripts))
    return f"""
ä½ æ˜¯ä¸€ä¸ªå°è¯´é…éŸ³ç”Ÿæˆagentï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·æä¾›çš„è¯­å¥ï¼Œåˆ†ææ¯ä¸€å¥è¯åº”è¯¥ç”¨ä»€ä¹ˆéŸ³è‰²é…éŸ³ã€‚

ä»¥ä¸‹æ˜¯å½“å‰çš„å°è¯´åŸæ–‡ï¼š
{novel_content}

ä»¥ä¸‹æ˜¯å½“å‰çš„éœ€è¦é…éŸ³çš„æ‰€æœ‰è¯­å¥ï¼š
{sentences}

å½“å‰çš„åœºæ™¯idä¸ºï¼š
{scene_id}

ä½ çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1. æ ¹æ®å°è¯´åŸæ–‡æ¢³ç†ä¸»è¦äººç‰©çš„æ€§åˆ«
2. æ ¹æ®è¯­å¥å†…å®¹å’Œè§’è‰²æ€§åˆ«ï¼Œä¸ºæ¯ä¸ªå¥å­åˆ†é…éŸ³è‰²

éŸ³è‰²åˆ†é…è§„åˆ™ï¼š
- **male**: ç”·æ€§è§’è‰²å¯¹è¯ã€ç”·æ€§å†…å¿ƒç‹¬ç™½ã€ä»¥ç”·æ€§è§†è§’çš„å†…å®¹
- **female**: å¥³æ€§è§’è‰²å¯¹è¯ã€å¥³æ€§å†…å¿ƒç‹¬ç™½ã€ä»¥å¥³æ€§è§†è§’çš„å†…å®¹  
- **narrator**: ç¯å¢ƒæè¿°ã€å™è¿°æ–‡å­—ã€æ— æ˜ç¡®æ€§åˆ«çš„å†…å®¹ã€å®¢è§‚æè¿°

è¯·ä¸ºæ¯ä¸ªå¥å­åˆ†é…åˆé€‚çš„éŸ³è‰²ç±»å‹ï¼Œç„¶åè°ƒç”¨ generate_audio_and_srt å·¥å…·ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ã€‚
"""


@talk_agent.tool
def generate_audio_and_srt(ctx: RunContext[TalkAgentDeps], segments: List[TalkAgentOutput]) -> str:
    """ä¸ºåˆ†æåçš„å¥å­ç”ŸæˆéŸ³é¢‘å’Œå­—å¹•æ–‡ä»¶"""
    scene_id = ctx.deps.scene_id

    # éªŒè¯æ•°æ®
    valid_segments = []
    for seg in segments:
        if seg.text.strip():
            valid_segments.append((seg.text.strip(), seg.voice_type))
    
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        audio_dir = "output/audio"
        srt_dir = "output/srt"
        os.makedirs(audio_dir, exist_ok=True)
        os.makedirs(srt_dir, exist_ok=True)
        
        # ç”Ÿæˆæ¯ä¸ªå¥å­çš„éŸ³é¢‘å’Œå­—å¹•
        audio_files, srt_files = generate_sentence_audio_and_srt(
            valid_segments, 
            "output", 
            scene_id
        )
        
        if not audio_files:
            return f"âŒ åœºæ™¯ {scene_id} éŸ³é¢‘ç”Ÿæˆå¤±è´¥"
        
        # åˆå¹¶éŸ³é¢‘æ–‡ä»¶
        merged_audio_path = os.path.join(audio_dir, f"scene_{scene_id}.wav")
        audio_result = merge_audio_files(audio_files, merged_audio_path)
        
        # åˆå¹¶SRTæ–‡ä»¶
        merged_srt_path = os.path.join(srt_dir, f"scene_{scene_id}.srt")
        srt_result = merge_srt_files(srt_files, merged_srt_path)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in audio_files + srt_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception:
                pass
        
        # ç»Ÿè®¡éŸ³è‰²ä½¿ç”¨æƒ…å†µ
        voice_stats = {}
        for _, voice_type in valid_segments:
            voice_stats[voice_type] = voice_stats.get(voice_type, 0) + 1
        
        stats_str = ", ".join([f"{voice}: {count}å¥" for voice, count in voice_stats.items()])
        
        return f"""âœ… åœºæ™¯ {scene_id} éŸ³é¢‘å’Œå­—å¹•ç”Ÿæˆå®Œæˆ:

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- å¥å­æ€»æ•°: {len(valid_segments)}
- éŸ³è‰²åˆ†å¸ƒ: {stats_str}

ğŸ“ è¾“å‡ºæ–‡ä»¶:
- éŸ³é¢‘: {merged_audio_path}
- å­—å¹•: {merged_srt_path}

ğŸ”§ å¤„ç†ç»“æœ:
- {audio_result}
- {srt_result}"""
        
    except Exception as e:
        return f"âŒ ç”ŸæˆéŸ³é¢‘å’Œå­—å¹•å¤±è´¥: {str(e)}"